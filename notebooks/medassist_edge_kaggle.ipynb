{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MedAssist-Edge: Offline Agentic Clinical Co-Pilot\n",
    "### Google MedGemma Kaggle Competition — Submission Notebook\n",
    "\n",
    "> **⚠️ Clinical Decision Support Only.** All outputs are AI-generated and must be reviewed by a qualified clinician before any clinical action.\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "| Agent | Role |\n",
    "|---|---|\n",
    "| SOAP Structuring Agent | Reorganises raw clinical notes into SOAP format |\n",
    "| Differential Diagnosis Agent | Generates ranked DDx with evidence |\n",
    "| Guideline Retrieval Agent (RAG) | Retrieves relevant local guideline passages |\n",
    "| Patient Explanation Agent | Produces plain-language patient summary |\n",
    "\n",
    "**Model: `google/medgemma-1.5-4b-it` — runs fully offline after download.**\n",
    "\n",
    "**API note:** MedGemma 1.5 uses `AutoProcessor` + `AutoModelForImageTextToText`\n",
    "and `processor.apply_chat_template()` — NOT `AutoTokenizer`/`AutoModelForCausalLM`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 1. Environment Setup"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers >= 4.50.0 required for Gemma 3 / MedGemma 1.5\n",
    "!pip install -q 'transformers>=4.50.0' accelerate sentencepiece bitsandbytes faiss-cpu sentence-transformers pymupdf\n",
    "print('Dependencies installed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, json, re, time, logging\n",
    "from pathlib import Path\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')\n",
    "logger = logging.getLogger('medassist')\n",
    "\n",
    "ROOT = Path('/kaggle/working')\n",
    "MODEL_CACHE  = ROOT / 'model_cache'\n",
    "EMBED_CACHE  = ROOT / 'embed_cache'\n",
    "VECTOR_STORE = ROOT / 'vector_store'\n",
    "GUIDELINES_DIR = ROOT / 'guidelines'\n",
    "\n",
    "for d in [MODEL_CACHE, EMBED_CACHE, VECTOR_STORE, GUIDELINES_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Paths configured.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 2. Load MedGemma 1.5 (Inference Engine)\n\nMedGemma 1.5 requires:\n- `AutoProcessor` (not `AutoTokenizer`)\n- `AutoModelForImageTextToText` (not `AutoModelForCausalLM`)\n- `processor.apply_chat_template()` for chat formatting\n- `do_sample=False` by default (Jan 23 2026 model card update)"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
    "\n",
    "MODEL_ID = 'google/medgemma-1.5-4b-it'\n",
    "USE_GPU  = torch.cuda.is_available()\n",
    "print(f'GPU available: {USE_GPU}')\n",
    "\n",
    "print(f'Loading processor: {MODEL_ID} ...')\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID, cache_dir=str(MODEL_CACHE))\n",
    "\n",
    "print('Loading model weights ...')\n",
    "t0 = time.time()\n",
    "if USE_GPU:\n",
    "    model = AutoModelForImageTextToText.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        cache_dir=str(MODEL_CACHE),\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map='auto',\n",
    "    )\n",
    "else:\n",
    "    # CPU path: int8 quantization via BitsAndBytes\n",
    "    bnb_cfg = BitsAndBytesConfig(load_in_8bit=True, bnb_4bit_compute_dtype=torch.float32)\n",
    "    model = AutoModelForImageTextToText.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        cache_dir=str(MODEL_CACHE),\n",
    "        quantization_config=bnb_cfg,\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "\n",
    "print(f'Model loaded in {time.time()-t0:.1f}s  device={model.device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Shared inference utilities ────────────────────────────────────────────────\n",
    "\n",
    "def generate(messages: list) -> str:\n",
    "    \"\"\"Run MedGemma 1.5 text-only inference from a messages list.\"\"\"\n",
    "    dtype = torch.bfloat16 if model.device.type != 'cpu' else torch.float32\n",
    "    inputs = processor.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "        return_tensors='pt',\n",
    "    ).to(model.device, dtype=dtype)\n",
    "    input_len = inputs['input_ids'].shape[-1]\n",
    "    with torch.inference_mode():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=768,\n",
    "            do_sample=False,          # greedy — deterministic (model card default)\n",
    "            repetition_penalty=1.15,\n",
    "        )\n",
    "    new_tokens = output_ids[0][input_len:]\n",
    "    return processor.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "\n",
    "def make_messages(system: str, user: str) -> list:\n",
    "    \"\"\"Build a MedGemma 1.5 messages list for text-only inference.\"\"\"\n",
    "    return [\n",
    "        {'role': 'system', 'content': [{'type': 'text', 'text': system}]},\n",
    "        {'role': 'user',   'content': [{'type': 'text', 'text': user}]},\n",
    "    ]\n",
    "\n",
    "\n",
    "def parse_json(raw: str) -> dict:\n",
    "    \"\"\"Extract JSON from model output, handling markdown fences.\"\"\"\n",
    "    text = re.sub(r'```(?:json)?', '', raw).strip().rstrip('`').strip()\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        m = re.search(r'\\{[\\s\\S]+\\}', text)\n",
    "        if m:\n",
    "            try: return json.loads(m.group())\n",
    "            except: pass\n",
    "    return {}\n",
    "\n",
    "\n",
    "print('Inference utilities ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 3. RAG Pipeline Setup"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write sample guideline content\n",
    "sample = \"\"\"IDIOPATHIC PULMONARY FIBROSIS (IPF) — ATS/ERS Guidelines 2022\n",
    "\n",
    "DIAGNOSIS: IPF requires UIP pattern on HRCT (honeycombing, traction bronchiectasis) plus\n",
    "exclusion of other ILD causes.\n",
    "\n",
    "WORKUP: PFTs (restrictive pattern: FVC↓, FEV1/FVC normal/↑, DLCO↓), 6MWT, echocardiography\n",
    "to exclude pulmonary hypertension. Serological panel (ANA, RF, anti-CCP) for CTD-ILD.\n",
    "\n",
    "MANAGEMENT: Antifibrotic therapy (nintedanib or pirfenidone) slows FVC decline.\n",
    "Supplemental O2 if SpO2 <88%. Lung transplant referral at diagnosis. Pulmonary rehab.\n",
    "AVOID: prednisone + azathioprine + NAC (shown harmful).\n",
    "\n",
    "MONITORING: FVC every 3-6 months; >10% decline = significant progression.\n",
    "DLCO every 6-12 months.\n",
    "\n",
    "COMMUNITY-ACQUIRED PNEUMONIA (CAP) — IDSA/ATS 2019\n",
    "\n",
    "DIAGNOSIS: New infiltrate + fever/cough/leukocytosis. CURB-65 guides site of care.\n",
    "CURB-65 ≥2 = consider hospitalisation.\n",
    "\n",
    "WORKUP: CXR, pulse oximetry. Blood cultures x2 before antibiotics in severe CAP.\n",
    "Urinary antigens (Legionella, S. pneumoniae) in severe cases. Procalcitonin for\n",
    "antibiotic duration guidance.\n",
    "\n",
    "MANAGEMENT: Empirical antibiotics (confirm with clinician and local formulary).\n",
    "Duration 5 days for non-severe CAP if clinically improving.\n",
    "\"\"\"\n",
    "(GUIDELINES_DIR / 'sample_guidelines.txt').write_text(sample)\n",
    "print('Sample guideline written.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss, numpy as np\n",
    "\n",
    "EMBED_MODEL = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "TOP_K = 4\n",
    "\n",
    "print(f'Loading embedding model: {EMBED_MODEL} ...')\n",
    "embed_model = SentenceTransformer(EMBED_MODEL, cache_folder=str(EMBED_CACHE))\n",
    "\n",
    "def chunk_text(text, source, size=512, overlap=64):\n",
    "    chunks, start, cid = [], 0, 0\n",
    "    while start < len(text):\n",
    "        t = text[start:start+size].strip()\n",
    "        if t: chunks.append({'source': source, 'chunk_id': cid, 'text': t}); cid += 1\n",
    "        start += size - overlap\n",
    "    return chunks\n",
    "\n",
    "all_chunks = []\n",
    "for f in GUIDELINES_DIR.glob('*.txt'):\n",
    "    all_chunks.extend(chunk_text(f.read_text(encoding='utf-8', errors='ignore'), f.name))\n",
    "\n",
    "embeddings = embed_model.encode(\n",
    "    [c['text'] for c in all_chunks], normalize_embeddings=True\n",
    ").astype(np.float32)\n",
    "\n",
    "index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "index.add(embeddings)\n",
    "faiss.write_index(index, str(VECTOR_STORE / 'guidelines.faiss'))\n",
    "with open(VECTOR_STORE / 'metadata.json', 'w') as f: json.dump(all_chunks, f)\n",
    "\n",
    "print(f'RAG index: {len(all_chunks)} chunks')\n",
    "\n",
    "def retrieve(query: str, k: int = TOP_K):\n",
    "    q = embed_model.encode([query], normalize_embeddings=True).astype(np.float32)\n",
    "    scores, idxs = index.search(q, k)\n",
    "    return [{'source': all_chunks[i]['source'], 'text': all_chunks[i]['text'],\n",
    "              'score': float(s)} for s, i in zip(scores[0], idxs[0]) if i >= 0]\n",
    "\n",
    "print('RAG pipeline ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Agent Definitions\n",
    "\n",
    "Each agent uses the MedGemma 1.5 `make_messages()` + `generate()` pattern.\n",
    "No raw `<start_of_turn>` strings — the processor handles chat formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── AGENT 1: SOAP Structuring ────────────────────────────────────────────────\n",
    "\n",
    "SOAP_SYSTEM = (\n",
    "    \"You are a clinical documentation assistant. Reorganise the clinician's raw \"\n",
    "    \"notes into SOAP format.\\n\\nRULES:\\n\"\n",
    "    \"1. Use ONLY information explicitly stated in the input. Do NOT add or infer anything.\\n\"\n",
    "    \"2. Plan section: SUGGESTIONS ONLY. Never write orders, prescriptions, or dosages.\\n\"\n",
    "    \"3. If a section is absent, write 'Not documented.'\\n\"\n",
    "    \"4. Output ONLY valid JSON:\\n\"\n",
    "    '{\"subjective\": \"...\", \"objective\": \"...\", \"assessment\": \"...\", \"plan_suggestions\": \"...\"}'\n",
    ")\n",
    "\n",
    "def soap_agent(clinical_notes, lab_results='', radiology_text='', age=None, sex=None):\n",
    "    user = (\n",
    "        f\"Organise into SOAP format.\\n\\n\"\n",
    "        f\"CLINICAL NOTES:\\n{clinical_notes}\\n\\n\"\n",
    "        f\"LAB RESULTS:\\n{lab_results or 'Not provided'}\\n\\n\"\n",
    "        f\"RADIOLOGY:\\n{radiology_text or 'Not provided'}\\n\\n\"\n",
    "        f\"DEMOGRAPHICS: Age={age or 'N/A'} Sex={sex or 'N/A'}\\n\\n\"\n",
    "        \"Return only the JSON object.\"\n",
    "    )\n",
    "    raw = generate(make_messages(SOAP_SYSTEM, user))\n",
    "    parsed = parse_json(raw)\n",
    "    return {\n",
    "        'subjective': parsed.get('subjective', 'Not documented.'),\n",
    "        'objective':  parsed.get('objective',  'Not documented.'),\n",
    "        'assessment': parsed.get('assessment', 'Not documented.'),\n",
    "        'plan_suggestions': parsed.get('plan_suggestions', 'Not documented.'),\n",
    "        'raw': raw\n",
    "    }\n",
    "\n",
    "print('Agent 1 (SOAP) defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── AGENT 2: Differential Diagnosis ─────────────────────────────────────────\n",
    "\n",
    "DDX_SYSTEM = (\n",
    "    \"You are a clinical reasoning assistant for differential diagnosis.\\n\\nRULES:\\n\"\n",
    "    \"1. Generate a RANKED differential (max 5 conditions).\\n\"\n",
    "    \"2. For each: condition, likelihood (High/Moderate/Low), supporting_features, against_features.\\n\"\n",
    "    \"3. Base reasoning ONLY on provided information. Use hedged language.\\n\"\n",
    "    \"4. Do NOT confirm any diagnosis. Do NOT recommend treatments or medications.\\n\"\n",
    "    \"5. Output ONLY valid JSON:\\n\"\n",
    "    '{\"diagnoses\": [{\"rank\":1,\"condition\":\"...\",\"likelihood\":\"High\",'\n",
    "    '\"supporting_features\":\"...\",\"against_features\":\"...\"}],'\n",
    "    '\"reasoning_summary\": \"...\"}'\n",
    ")\n",
    "\n",
    "def ddx_agent(soap, age=None, sex=None):\n",
    "    user = (\n",
    "        f\"Generate ranked differential.\\n\\n\"\n",
    "        f\"SUBJECTIVE: {soap['subjective']}\\n\"\n",
    "        f\"OBJECTIVE: {soap['objective']}\\n\"\n",
    "        f\"ASSESSMENT: {soap['assessment']}\\n\"\n",
    "        f\"DEMOGRAPHICS: Age={age or 'N/A'} Sex={sex or 'N/A'}\"\n",
    "    )\n",
    "    raw = generate(make_messages(DDX_SYSTEM, user))\n",
    "    parsed = parse_json(raw)\n",
    "    return {\n",
    "        'diagnoses': parsed.get('diagnoses', [])[:5],\n",
    "        'reasoning_summary': parsed.get('reasoning_summary', ''),\n",
    "        'raw': raw\n",
    "    }\n",
    "\n",
    "print('Agent 2 (DDx) defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── AGENT 3: Guideline Retrieval (RAG) ──────────────────────────────────────\n",
    "\n",
    "GUIDELINE_SYSTEM = (\n",
    "    \"You are a clinical guideline synthesis assistant.\\n\\nRULES:\\n\"\n",
    "    \"1. Synthesise ONLY from provided retrieved excerpts. Do NOT use training knowledge.\\n\"\n",
    "    \"2. Attribute every recommendation to its source.\\n\"\n",
    "    \"3. Organise by category: Workup, Management, Monitoring, Follow-up.\\n\"\n",
    "    \"4. Do NOT recommend specific drug doses. Do NOT issue clinical orders.\\n\"\n",
    "    \"5. Output ONLY valid JSON:\\n\"\n",
    "    '{\"recommendations\":[{\"category\":\"...\",\"recommendation\":\"...\",'\n",
    "    '\"source\":\"...\",\"confidence\":\"Direct|Inferred|Low-evidence\"}],'\n",
    "    '\"retrieved_sources\":[\"...\"]}'\n",
    ")\n",
    "\n",
    "def guideline_agent(soap, ddx):\n",
    "    conditions = ', '.join(d['condition'] for d in ddx['diagnoses'][:3])\n",
    "    query = f\"{soap['assessment']} {conditions} {soap['subjective'][:200]}\"\n",
    "    chunks = retrieve(query)\n",
    "    chunk_block = '\\n\\n---\\n\\n'.join(\n",
    "        f\"[{i+1}] SOURCE: {c['source']}\\n{c['text']}\" for i, c in enumerate(chunks)\n",
    "    ) or 'No guideline excerpts retrieved.'\n",
    "    user = (\n",
    "        f\"CLINICAL SUMMARY:\\nAssessment: {soap['assessment']}\\nTop DDx: {conditions}\\n\\n\"\n",
    "        f\"RETRIEVED GUIDELINE EXCERPTS:\\n{chunk_block}\\n\\n\"\n",
    "        \"Synthesise recommendations from excerpts ONLY.\"\n",
    "    )\n",
    "    raw = generate(make_messages(GUIDELINE_SYSTEM, user))\n",
    "    parsed = parse_json(raw)\n",
    "    return {\n",
    "        'recommendations': parsed.get('recommendations', []),\n",
    "        'retrieved_sources': [c['source'] for c in chunks],\n",
    "        'raw': raw\n",
    "    }\n",
    "\n",
    "print('Agent 3 (Guidelines) defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── AGENT 4: Patient Explanation ────────────────────────────────────────────\n",
    "\n",
    "PATIENT_SYSTEM = (\n",
    "    \"You are a patient communication assistant.\\n\\nRULES:\\n\"\n",
    "    \"1. Write at 6th-grade reading level. Explain medical terms in brackets.\\n\"\n",
    "    \"2. Do NOT confirm any diagnosis. Say 'your doctor is considering...'\\n\"\n",
    "    \"3. Do NOT mention medications or specific procedures.\\n\"\n",
    "    \"4. Be warm, empathetic, non-alarming.\\n\"\n",
    "    \"5. End with next-steps encouraging patient to speak with their doctor.\\n\"\n",
    "    \"6. Output ONLY valid JSON:\\n\"\n",
    "    '{\"summary\":\"...\",\"key_points\":[\"...\"],\"next_steps_suggestion\":\"...\"}'\n",
    ")\n",
    "\n",
    "def patient_agent(soap, ddx):\n",
    "    ddx_summary = '\\n'.join(\n",
    "        f\"- {d['condition']} is being considered ({d['likelihood']} likelihood)\"\n",
    "        for d in ddx['diagnoses'][:3]\n",
    "    ) or 'No specific conditions documented yet.'\n",
    "    user = (\n",
    "        f\"Translate to plain patient-friendly language.\\n\\n\"\n",
    "        f\"SYMPTOMS/FINDINGS: {soap['subjective']}\\n{soap['objective']}\\n\\n\"\n",
    "        f\"WORKING ASSESSMENT: {soap['assessment']}\\n\\n\"\n",
    "        f\"POSSIBILITIES (not confirmed): {ddx_summary}\"\n",
    "    )\n",
    "    raw = generate(make_messages(PATIENT_SYSTEM, user))\n",
    "    parsed = parse_json(raw)\n",
    "    return {\n",
    "        'summary': parsed.get('summary', 'Please speak with your doctor.'),\n",
    "        'key_points': parsed.get('key_points', [])[:5],\n",
    "        'next_steps_suggestion': parsed.get('next_steps_suggestion', 'Please discuss with your doctor.'),\n",
    "        'raw': raw\n",
    "    }\n",
    "\n",
    "print('Agent 4 (Patient) defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 5. Full Pipeline — Demo Case\n\n**Case:** 45-year-old female, progressive dyspnoea, restrictive spirometry, HRCT honeycombing"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEMO_CASE = dict(\n",
    "    clinical_notes=(\n",
    "        \"45-year-old female presenting with a 3-week history of progressive dyspnea on exertion, \"\n",
    "        \"dry cough, and fatigue. No fever, no chest pain, no haemoptysis. Non-smoker. No recent \"\n",
    "        \"travel. Works in a textile factory. On examination: RR 22/min, SpO2 91% on room air, \"\n",
    "        \"bilateral fine inspiratory crackles at lung bases. No clubbing, no peripheral oedema.\"\n",
    "    ),\n",
    "    lab_results=(\n",
    "        \"CBC: WBC 8.2, Hgb 11.4 g/dL, Plt 310. LDH 280 U/L (ref <225). \"\n",
    "        \"ESR 68 mm/hr. CRP 2.1 mg/dL. Spirometry: FVC 68% predicted, FEV1/FVC 0.81.\"\n",
    "    ),\n",
    "    radiology_text=(\n",
    "        \"HRCT Chest: Bilateral ground-glass opacities in the lower lobes with honeycombing \"\n",
    "        \"and traction bronchiectasis. No pleural effusion. No lymphadenopathy.\"\n",
    "    ),\n",
    "    age=45,\n",
    "    sex='female'\n",
    ")\n",
    "print('Demo case loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCLAIMER = (\n",
    "    '⚠️  CLINICAL DECISION SUPPORT ONLY — AI-generated output must be reviewed '\n",
    "    'by a qualified clinician. Not a diagnosis, prescription, or medical order.'\n",
    ")\n",
    "\n",
    "t_start = time.time()\n",
    "print('=' * 60)\n",
    "\n",
    "print('[1/4] SOAP Structuring Agent ...')\n",
    "t0 = time.time(); soap = soap_agent(**DEMO_CASE)\n",
    "print(f'  {time.time()-t0:.1f}s')\n",
    "\n",
    "print('[2/4] Differential Diagnosis Agent ...')\n",
    "t0 = time.time(); ddx = ddx_agent(soap, age=45, sex='female')\n",
    "print(f'  {time.time()-t0:.1f}s — {len(ddx[\"diagnoses\"])} entries')\n",
    "\n",
    "print('[3/4] Guideline Retrieval Agent (RAG) ...')\n",
    "t0 = time.time(); guidelines = guideline_agent(soap, ddx)\n",
    "print(f'  {time.time()-t0:.1f}s — {len(guidelines[\"recommendations\"])} recs')\n",
    "\n",
    "print('[4/4] Patient Explanation Agent ...')\n",
    "t0 = time.time(); patient = patient_agent(soap, ddx)\n",
    "print(f'  {time.time()-t0:.1f}s')\n",
    "\n",
    "print(f'\\nTotal pipeline: {time.time()-t_start:.1f}s')\n",
    "print('=' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 6. Results"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DISCLAIMER); print()\n",
    "print('━'*60); print('AGENT 1 — SOAP NOTE'); print('━'*60)\n",
    "for k, v in [('SUBJECTIVE', soap['subjective']), ('OBJECTIVE', soap['objective']),\n",
    "              ('ASSESSMENT', soap['assessment']), ('PLAN SUGGESTIONS', soap['plan_suggestions'])]:\n",
    "    print(f'\\n{k}:\\n{v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('━'*60); print('AGENT 2 — DIFFERENTIAL DIAGNOSIS'); print('━'*60)\n",
    "for d in ddx['diagnoses']:\n",
    "    print(f\"\\n{d['rank']}. {d['condition']} — {d['likelihood']}\")\n",
    "    print(f\"   FOR    : {d['supporting_features']}\")\n",
    "    print(f\"   AGAINST: {d['against_features']}\")\n",
    "print(f\"\\nReasoning: {ddx['reasoning_summary']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('━'*60); print('AGENT 3 — GUIDELINE RECOMMENDATIONS (RAG)'); print('━'*60)\n",
    "print(f\"Sources: {', '.join(guidelines['retrieved_sources'])}\\n\")\n",
    "for r in guidelines['recommendations']:\n",
    "    print(f\"[{r['category']}] ({r['confidence']}) {r['recommendation']}\")\n",
    "    print(f\"  Source: {r['source']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('━'*60); print('AGENT 4 — PATIENT EXPLANATION'); print('━'*60)\n",
    "print(patient['summary'])\n",
    "print('\\nKey Points:')\n",
    "for i, p in enumerate(patient['key_points'], 1): print(f'  {i}. {p}')\n",
    "print(f\"\\nNext Steps: {patient['next_steps_suggestion']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "| Metric | Value |\n",
    "|---|---|\n",
    "| Model | google/medgemma-1.5-4b-it |\n",
    "| Model class | AutoModelForImageTextToText |\n",
    "| Processor | AutoProcessor |\n",
    "| Quantization | int8 CPU / bfloat16 GPU |\n",
    "| Agents executed | 4 |\n",
    "| RAG | Local FAISS index |\n",
    "| Internet dependency | None at inference time |\n",
    "| Diagnoses confirmed | 0 — by design |\n",
    "| Prescriptions issued | 0 — by design |\n",
    "\n",
    "---\n",
    "\n",
    "> **Research and competition use only. Not clinically validated. Not approved for clinical use.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.10.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
